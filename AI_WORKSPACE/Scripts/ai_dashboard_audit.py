# üß† AI-Powered Dashboard Audit Script (Enhanced)
# Safe, dynamic scoring with intelligent recommendations
# Generated by Edge Copilot guidance and GitHub Copilot implementation

import yaml
import os
import json
from datetime import datetime
from pathlib import Path

class DashboardAuditor:
    def __init__(self, dashboard_root="/config/dashboards/"):
        self.dashboard_root = dashboard_root
        self.log_dir = "/config/AI_WORKSPACE/logs/"
        self.ensure_log_dir()
    
    def ensure_log_dir(self):
        """Ensure log directory exists"""
        os.makedirs(self.log_dir, exist_ok=True)
    
    def analyze_dashboard(self, file_path):
        """Analyze a single dashboard file for complexity and optimization opportunities"""
        try:
            with open(file_path, 'r') as f:
                data = yaml.safe_load(f)
        except Exception as e:
            return 0, [f"YAML parsing error: {e}"]
        
        if not data:
            return 0, ["Empty or invalid YAML file"]
        
        score = 0
        tips = []
        performance_issues = []
        
        # Analyze views
        views = data.get('views', [])
        if not views:
            # Single view dashboard
            cards = data.get('cards', [])
            score, view_tips = self._analyze_cards(cards, "main")
            tips.extend(view_tips)
        else:
            # Multi-view dashboard
            for i, view in enumerate(views):
                view_name = view.get('title', f'view_{i}')
                cards = view.get('cards', [])
                view_score, view_tips = self._analyze_cards(cards, view_name)
                score += view_score
                tips.extend(view_tips)
                
                # Check for excessive views
                if len(views) > 5:
                    performance_issues.append(f"Dashboard has {len(views)} views - consider splitting into multiple dashboards")
        
        # Performance analysis
        if score > 150:
            performance_issues.append("High complexity score - dashboard may load slowly")
        elif score > 100:
            performance_issues.append("Moderate complexity - consider optimization")
        
        # Combine tips and performance issues
        all_recommendations = tips + performance_issues
        
        return score, all_recommendations
    
    def _analyze_cards(self, cards, view_name):
        """Analyze cards within a view"""
        score = 0
        tips = []
        
        for i, card in enumerate(cards):
            card_type = card.get('type', 'unknown')
            
            # Base scoring
            score += 1
            
            # Nested card detection
            if 'cards' in card:
                nested_count = len(card['cards'])
                score += nested_count * 2
                tips.append(f"{view_name}: Nested card with {nested_count} children - consider flattening")
            
            # Custom card analysis
            if card_type.startswith('custom:'):
                score += 3
                tips.append(f"{view_name}: Custom card '{card_type}' detected - verify performance impact")
                
                # Specific custom card warnings
                if card_type == 'custom:swipe-card':
                    tips.append(f"{view_name}: Swipe card may cause reload loops - monitor performance")
                elif card_type == 'custom:layout-card':
                    tips.append(f"{view_name}: Layout card detected - check for rendering overhead")
                elif card_type == 'custom:auto-entities':
                    tips.append(f"{view_name}: Auto-entities card - ensure entity_filter is optimized")
            
            # Entity overload detection
            entities = card.get('entities', [])
            if len(entities) > 20:
                score += 10
                tips.append(f"{view_name}: Card has {len(entities)} entities - consider pagination or filtering")
            elif len(entities) > 10:
                score += 5
                tips.append(f"{view_name}: Card has {len(entities)} entities - monitor load time")
            
            # Chart and graph analysis
            if card_type in ['history-graph', 'statistics-graph', 'sensor']:
                score += 2
                entities_count = len(entities)
                if entities_count > 5:
                    tips.append(f"{view_name}: Graph with {entities_count} entities may be slow")
            
            # Map card detection
            if card_type == 'map':
                score += 5
                tips.append(f"{view_name}: Map card detected - high rendering cost")
        
        return score, tips
    
    def audit_all_dashboards(self):
        """Audit all dashboards and generate comprehensive report"""
        if not os.path.exists(self.dashboard_root):
            return {"error": "Dashboard directory not found"}
        
        results = {}
        total_score = 0
        all_tips = []
        dashboard_count = 0
        
        for root, dirs, files in os.walk(self.dashboard_root):
            for file in files:
                if file.endswith('.yaml'):
                    file_path = os.path.join(root, file)
                    rel_path = os.path.relpath(file_path, self.dashboard_root)
                    
                    score, tips = self.analyze_dashboard(file_path)
                    results[rel_path] = {
                        'score': score,
                        'tips': tips,
                        'grade': self._calculate_grade(score)
                    }
                    
                    total_score += score
                    all_tips.extend([f"{rel_path}: {tip}" for tip in tips])
                    dashboard_count += 1
        
        # Generate summary
        avg_score = total_score / dashboard_count if dashboard_count > 0 else 0
        overall_grade = self._calculate_grade(avg_score)
        
        summary = {
            'timestamp': datetime.now().isoformat(),
            'dashboard_count': dashboard_count,
            'total_score': total_score,
            'average_score': round(avg_score, 1),
            'overall_grade': overall_grade,
            'high_complexity_dashboards': [name for name, data in results.items() if data['score'] > 150],
            'top_recommendations': all_tips[:10],  # Top 10 recommendations
            'detailed_results': results
        }
        
        return summary
    
    def _calculate_grade(self, score):
        """Calculate performance grade based on complexity score"""
        if score < 30:
            return 'A+'
        elif score < 50:
            return 'A'
        elif score < 80:
            return 'B'
        elif score < 120:
            return 'C'
        else:
            return 'D'
    
    def generate_optimization_recommendation(self, score, error_count=0):
        """Generate intelligent optimization recommendation"""
        if error_count > 15:
            return f"üö® CRITICAL: {error_count} frontend errors demand immediate attention. Check Recovery Dashboard."
        elif error_count > 10:
            return f"‚ö†Ô∏è HIGH PRIORITY: {error_count} errors detected. Review dashboard complexity (score: {score})."
        elif error_count > 5:
            return f"üìã MODERATE: {error_count} errors present. Consider dashboard optimization (score: {score})."
        elif score > 150:
            return f"üî• COMPLEXITY ALERT: Score {score} is very high. Simplify views and reduce nested cards."
        elif score > 100:
            return f"‚ö° OPTIMIZATION OPPORTUNITY: Score {score}. Consider card consolidation and entity reduction."
        elif score > 50:
            return f"üìä MONITOR: Score {score} is moderate. Watch for performance trends."
        elif error_count > 0:
            return f"üîç MINOR ISSUES: {error_count} errors detected. System stable but monitor trends."
        else:
            return f"üèÜ EXCELLENT: Score {score}, zero errors. System optimally configured."
    
    def save_audit_log(self, summary):
        """Save audit results to log file"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_file = os.path.join(self.log_dir, f'dashboard_audit_{timestamp}.log')
        
        with open(log_file, 'w') as f:
            json.dump(summary, f, indent=2)
        
        # Also save as latest
        latest_file = os.path.join(self.log_dir, 'latest_dashboard_audit.json')
        with open(latest_file, 'w') as f:
            json.dump(summary, f, indent=2)
        
        return log_file

def main():
    """Main execution function"""
    auditor = DashboardAuditor()
    summary = auditor.audit_all_dashboards()
    
    if 'error' in summary:
        print(f"ERROR: {summary['error']}")
        return
    
    # Save results
    log_file = auditor.save_audit_log(summary)
    
    # Print summary
    print(f"Dashboard Audit Complete - {summary['timestamp']}")
    print(f"Dashboards Analyzed: {summary['dashboard_count']}")
    print(f"Total Complexity Score: {summary['total_score']}")
    print(f"Average Score: {summary['average_score']}")
    print(f"Overall Grade: {summary['overall_grade']}")
    
    if summary['high_complexity_dashboards']:
        print(f"High Complexity Dashboards: {', '.join(summary['high_complexity_dashboards'])}")
    
    print(f"Top Recommendations:")
    for rec in summary['top_recommendations']:
        print(f"  - {rec}")
    
    print(f"Full report saved to: {log_file}")

if __name__ == "__main__":
    main()